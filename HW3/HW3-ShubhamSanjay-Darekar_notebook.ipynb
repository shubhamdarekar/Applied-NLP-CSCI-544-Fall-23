{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 544 - Homework Assignment No 3\n",
    "------------\n",
    "\n",
    "`Version`   : 1.0\\\n",
    "`Editor`    : Shubham Sanjay Darekar\\\n",
    "`Date`      : 10/19/2023 \\\n",
    "`Execution Time`: ~ 13 min\n",
    "\n",
    "### Final Accuracy Values as of 10/19\n",
    "\n",
    "| Model | Input Features |Accuracy |\n",
    "| ----- | ----- | ----- |\n",
    "| Perceptron | Mean Word2Vec | 82.5 %|\n",
    "|  | TF_IDF |89.52 % |\n",
    "| SVM | Mean Word2Vec |84.28 %|\n",
    "|  | TF_IDF |90.83 % |\n",
    "| FNN | Mean Word2Vec | 86.48 %|\n",
    "| FNN | First 10 Word2Vec | 82.13 %|\n",
    "| Simple RNN | First 10 Word2Vec |83.19 %|\n",
    "| Gated RNN | First 10 Word2Vec |83.69 %|\n",
    "| LSTM | First 10 Word2Vec |83.27 %|\n",
    "\n",
    "### Initial tasks\n",
    "##### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords,strip_punctuation,strip_tags\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "def eval(actual,predicted):\n",
    "    acc = accuracy_score(actual,predicted)\n",
    "    print('Accuracy = '+str(round(acc,4)))\n",
    "    \n",
    "## Approx run time - 15s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brief description of usage of all the libraries imported \n",
    "1. Pandas - Used to read and manupulate the data using dataframe\n",
    "2. NumPy - Used to manupulate the numeric values in the dataset\n",
    "3. Torch - This module has implementation of all the Neural Network models used in solution\n",
    "4. Gensim - This module has implementation of Word2vec, which is used for generating doc embeddings\n",
    "7. Sklearn - This module has implementation of all the ML models used in the solution\n",
    "\n",
    "##### Defining the device to be used (Ran on Machine with Intel i7 8th Gen, NVIDIA GPU GTX 1050Ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data read_csv method from pandas is used.\\\n",
    "`Parameters`:\n",
    "> 1. sep ~ Seperater - \\t as the values are tab seperated\n",
    "> 2. engine ~ Using python engine to avoid unsupported format by C engine - `Ref` - https://stackoverflow.com/questions/52774459/engines-in-python-pandas-read-csv\n",
    "> 3. quoting ~ Set to 3 i.e none to control the quoting field behaviour - `Ref` - https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
    "> 4. on_bad_lines ~ Skip option skips the bad lines while reading the dataset\n",
    "\n",
    "As the review headline and review body are the fields using in classifier and star rating is the field used to label the categories just slicing those fields. The review headline and review body are concatenated with a space in between as headline is helping in model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_ratings = pd.read_csv(\"D:\\\\Applied NLP\\\\HW1\\\\amazon_reviews_us_Office_Products_v1_00.tsv\", sep='\\t', engine=\"python\", quoting=3, on_bad_lines='skip')\n",
    "reviews_ratings = pd.read_csv(\".\\\\data.tsv\", sep='\\t', engine=\"python\", quoting=3, on_bad_lines='skip')\n",
    "reviews_ratings = reviews_ratings[['review_headline','review_body','star_rating']]\n",
    "\n",
    "## Filling Na values with blank as one of the column from headline and body might contain useful data\n",
    "reviews_ratings['review_headline'].fillna(\"\", inplace=True) \n",
    "reviews_ratings['review_body'].fillna(\"\", inplace=True)\n",
    "\n",
    "\n",
    "reviews_ratings['review_headline_body'] = reviews_ratings['review_headline'] + \" \" + reviews_ratings['review_body']\n",
    "\n",
    "#Random selection of 50,000 rows from each class\n",
    "df_class1 = reviews_ratings[reviews_ratings['star_rating']>3].sample(n=50000, random_state=34) # setting the random state to reproduce the output\n",
    "df_class1['Class'] = 1 ## Class with higher rating\n",
    "df_class2 = reviews_ratings[reviews_ratings['star_rating']<=3].sample(n=50000, random_state=34) # setting the random state to reproduce the output\n",
    "df_class2['Class'] = 0 ## Class with lower rating\n",
    "\n",
    "reviews_ratings_final = pd.concat([df_class1, df_class2], ignore_index=True, sort=False).reset_index(drop=True) #concating the classes\n",
    "\n",
    "## Approx run time - 1min 20s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the dataset to parquet file in order to save computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ratings_final.to_parquet(\"AmazonReviewsProcessed.parquet\")\n",
    "##Reading when required\n",
    "# reviews_ratings_final = pd.read_parquet(\"AmazonReviewsProcessed.parquet\")\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Word Embeddings\n",
    "\n",
    "#### Task (a) : Generation of similarities using pretrained “word2vec-google-news-300”\n",
    "\n",
    "`Ref` - https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the pretrained model\n",
    "wv_googleNews = api.load('word2vec-google-news-300')\n",
    "\n",
    "## Approx run time - 1min 20s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Checking semantic similarity for the King woman man example (Trying both all lower case and first letter capital answers as they are generating different results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Queen', 0.4929387867450714), ('Tupou_V.', 0.45174285769462585), ('Oprah_BFF_Gayle', 0.4422132968902588), ('Jackson', 0.440250426530838), ('NECN_Alison', 0.4331282675266266), ('Whitfield', 0.42834725975990295), ('Ida_Vandross', 0.42084527015686035), ('prosecutor_Dan_Satterberg', 0.420758992433548), ('martin_Luther_King', 0.42059651017189026), ('Coretta_King', 0.4202733635902405)]\n",
      "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593831062317), ('monarchy', 0.5087411999702454)]\n",
      "\n",
      "Output for [\"King\",\"Woman\"],\"Man\":Queen\n",
      "Output for [\"king\",\"woman\"],\"man\":queen\n"
     ]
    }
   ],
   "source": [
    "out1 = wv_googleNews.most_similar([\"King\",\"Woman\"],\"Man\")\n",
    "out2 = wv_googleNews.most_similar([\"king\",\"woman\"],\"man\")\n",
    "\n",
    "print(out1)\n",
    "print(out2)\n",
    "\n",
    "print('\\nOutput for [\"King\",\"Woman\"],\"Man\":' + out1[0][0])\n",
    "print('Output for [\"king\",\"woman\"],\"man\":' + out2[0][0])\n",
    "\n",
    "## Approx run time - 4s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `--> The pretrained model outputs Queen in both the cases with around 70% confidence and 40% in case if first letter capital`\n",
    "\n",
    "\n",
    "Trying out more similar examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('winter', 0.5788487195968628), ('snowstorm', 0.5582926869392395), ('heavy_snows', 0.5265657305717468), ('heavy_snowfall', 0.509794294834137), ('spring', 0.5073685646057129), ('snowstorms', 0.5022254586219788), ('snowfalls', 0.5017697215080261), ('heavy_snowfalls', 0.4968310594558716), ('snows', 0.4946661591529846), ('wintry_weather', 0.49291324615478516)]\n",
      "[('Winter', 0.5541607737541199), ('Nellis_Stiffler', 0.4493800103664398), ('Rockcliff_presently_controls', 0.4162043631076813), ('Spring', 0.4159749150276184), ('Fall', 0.41410258412361145), ('Tommy_Wirkola_Dead', 0.411090224981308), ('LeBron_trudging', 0.4106634259223938), ('summer', 0.4085595905780792), ('winter', 0.40848907828330994), ('WInter', 0.40385767817497253)]\n",
      "\n",
      "Output for [\"summer\",\"snow\"],\"sun\":winter\n",
      "Output for [\"Summer\",\"Snow\"],\"Sun\":Winter\n",
      "\n",
      "\n",
      "[('movies', 0.5982824563980103), ('cinema', 0.5235484838485718), ('multiplex', 0.5145583748817444), ('cineplex', 0.4867870509624481), ('films', 0.4812585711479187), ('Actors_Equity_arranged', 0.47572818398475647), ('studio_backlot', 0.47454366087913513), ('film', 0.4735766649246216), ('moviehouse', 0.47016361355781555), ('Movie', 0.45994073152542114)]\n",
      "[('Public_Library', 0.5193747878074646), ('Branch_Library', 0.48947563767433167), ('Cinema', 0.48922380805015564), ('Movies', 0.48105770349502563), ('library', 0.4696941077709198), ('LIbrary', 0.44179263710975647), ('Libary', 0.43679797649383545), ('MOVIES_IN', 0.4302363097667694), ('Steven_Goldmann', 0.4271371066570282), ('Film', 0.42455431818962097)]\n",
      "\n",
      "Output for [\"library\",\"movie\"],\"book\":movies\n",
      "Output for [\"Library\",\"Movie\"],\"Book\":Public_Library\n",
      "\n",
      "\n",
      "[('crunchies', 0.5558125376701355), ('brussel_sprout', 0.5546073913574219), ('carrot_slices', 0.552364706993103), ('cheesy_polenta', 0.5433920621871948), ('sprouting_broccoli', 0.5425909161567688), ('tomatoe', 0.5401697754859924), ('peas_pears', 0.5396046042442322), ('eggwhite', 0.5394821166992188), ('chestnut_purée', 0.5392216444015503), ('rollatini', 0.5383666157722473)]\n",
      "\n",
      "Output for [\"apple\",\"brocolli\"],\"fruit\":crunchies\n"
     ]
    }
   ],
   "source": [
    "out1 = wv_googleNews.most_similar([\"summer\",\"snow\"],\"sun\")\n",
    "out2 = wv_googleNews.most_similar([\"Summer\",\"Snow\"],\"Sun\")\n",
    "\n",
    "print(out1)\n",
    "print(out2)\n",
    "\n",
    "print('\\nOutput for [\"summer\",\"snow\"],\"sun\":' + out1[0][0])\n",
    "print('Output for [\"Summer\",\"Snow\"],\"Sun\":' + out2[0][0])\n",
    "print(\"\\n\")\n",
    "\n",
    "out1 = wv_googleNews.most_similar([\"library\",\"movie\"],\"book\")\n",
    "out2 = wv_googleNews.most_similar([\"Library\",\"Movie\"],\"Book\")\n",
    "\n",
    "print(out1)\n",
    "print(out2)\n",
    "\n",
    "print('\\nOutput for [\"library\",\"movie\"],\"book\":' + out1[0][0])\n",
    "print('Output for [\"Library\",\"Movie\"],\"Book\":' + out2[0][0])\n",
    "print(\"\\n\")\n",
    "\n",
    "out1 = wv_googleNews.most_similar([\"apple\",\"brocolli\"],\"fruits\")\n",
    "# out2 = wv_googleNews.most_similar([\"Apple\",\"Brocolli\"],\"Fruit\") ## Capital lettered word not present in vocab\n",
    "\n",
    "print(out1)\n",
    "# print(out2)\n",
    "\n",
    "print('\\nOutput for [\"apple\",\"brocolli\"],\"fruit\":' + out1[0][0])\n",
    "## Approx run time - 2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Conclusion` - The pretrained model performs quite well while generating similarities with positive and negative words\n",
    "\n",
    "2. Checking similarities between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities between excellent and outstanding : 0.55674857\n",
      "Similarities between beautiful and gorgeous : 0.8353004\n",
      "Similarities between fast and quick : 0.57016057\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarities between excellent and outstanding : \" + str(wv_googleNews.similarity('excellent', 'outstanding')))\n",
    "\n",
    "print(\"Similarities between beautiful and gorgeous : \" + str(wv_googleNews.similarity('beautiful', 'gorgeous')))\n",
    "\n",
    "print(\"Similarities between fast and quick : \" + str(wv_googleNews.similarity('fast', 'quick')))\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Conclusion` - The pretrained model performs quite well while generating similarities similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task (b): Generating word2vec vectors with the amazon reviews processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function is used to tokenize and preprocess the string using gensim's preprocess_string function (https://piazza.com/class/llm91seaknw3j6/post/314)\n",
    "Using the following filters:\n",
    "remove_stopwords- Removes the stopwords from the text\n",
    "strip_punctuation- Removes the punctions\n",
    "strip_tags-Removes the HTML and XML tags \n",
    "\n",
    "Ref - https://radimrehurek.com/gensim/parsing/preprocessing.html#gensim.parsing.preprocessing.preprocess_documents\n",
    "\"\"\"\n",
    "def processed_data_tokens(s:any):\n",
    "    s = preprocess_string(s,[remove_stopwords,strip_punctuation,strip_tags])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying the above preprocessing function\n",
    "reviews_ratings_final['review_headline_body_processed'] = reviews_ratings_final['review_headline_body'].apply(processed_data_tokens)\n",
    "\n",
    "\n",
    "## Dropping the reviews with zero length after preprocessing to remove the bad input\n",
    "reviews_ratings_final.drop(reviews_ratings_final[reviews_ratings_final['review_headline_body_processed'].apply(len) == 0].index,inplace=True)\n",
    "reviews_ratings_final.reset_index(drop=True,inplace=True)\n",
    "## Approx run time - 5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the word2vec word embeddings with this processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_own_dataset = Word2Vec(sentences=reviews_ratings_final['review_headline_body_processed'],\n",
    "                             vector_size=300, ## Size of embedding\n",
    "                             window=13, ## Size of the window\n",
    "                             min_count=9) ## Minimum word count \n",
    "\n",
    "## Approx run time - 35s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Checking semantic similarity for the King woman man example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Superior', 0.8282687664031982), ('excelente', 0.8080716729164124), ('bien', 0.8069323897361755), ('Excelente', 0.7974227666854858), ('muy', 0.7970524430274963), ('Travel', 0.7931545972824097), ('producto', 0.7846735715866089), ('mi', 0.7805020213127136), ('gracias', 0.7776635885238647), ('buen', 0.7773924469947815)]\n",
      "[('Living', 0.7969949841499329), ('Appointment', 0.7810559272766113), ('Keeper', 0.7752621173858643), ('Academic', 0.7694739699363708), ('Future', 0.7676205039024353), ('Height', 0.7651114463806152), ('USER', 0.7627473473548889), ('Park', 0.7623292207717896), ('Bound', 0.7588553428649902), ('LAPTOP', 0.7548812627792358)]\n",
      "\n",
      "Output for [\"King\",\"Woman\"],\"Man\":Superior\n",
      "Output for [\"king\",\"woman\"],\"man\":Living\n"
     ]
    }
   ],
   "source": [
    "out1 = model_own_dataset.wv.most_similar([\"King\",\"Woman\"],\"Man\")\n",
    "out2 = model_own_dataset.wv.most_similar([\"king\",\"woman\"],\"man\")\n",
    "\n",
    "print(out1)\n",
    "print(out2)\n",
    "\n",
    "print('\\nOutput for [\"King\",\"Woman\"],\"Man\":' + out1[0][0])\n",
    "print('Output for [\"king\",\"woman\"],\"man\":' + out2[0][0])\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `--> The self trained model does not output the expected Queen in both the cases`\n",
    "\n",
    "\n",
    "Trying out more similar examples (The part of code is commented as the vocab list is limited in self trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stocking', 0.6913248300552368), ('bookstore', 0.6752166152000427), ('graduation', 0.6717312932014465), ('teacher', 0.6686440706253052), ('Influenster', 0.6678372621536255), ('graduate', 0.6612659096717834), ('6th', 0.6597033739089966), ('schooling', 0.6536543965339661), ('stuffer', 0.6512351632118225), ('university', 0.6471811532974243)]\n",
      "\n",
      "Output for [\"summer\",\"snow\"],\"sun\":stocking\n",
      "\n",
      "\n",
      "[('movies', 0.7601059079170227), ('theater', 0.7110424637794495), ('presentations', 0.6842615008354187), ('watching', 0.6741736531257629), ('darkened', 0.6517071723937988), ('PowerPoint', 0.638480007648468), ('gaming', 0.6290944218635559), ('ray', 0.6210340857505798), ('games', 0.6130325198173523), ('Netflix', 0.6061313152313232)]\n",
      "\n",
      "Output for [\"library\",\"movie\"],\"book\":movies\n",
      "\n",
      "\n",
      "[('movies', 0.7601059079170227), ('theater', 0.7110424637794495), ('presentations', 0.6842615008354187), ('watching', 0.6741736531257629), ('darkened', 0.6517071723937988), ('PowerPoint', 0.638480007648468), ('gaming', 0.6290944218635559), ('ray', 0.6210340857505798), ('games', 0.6130325198173523), ('Netflix', 0.6061313152313232)]\n",
      "\n",
      "Output for [\"apple\",\"brocolli\"],\"fruit\": NA\n"
     ]
    }
   ],
   "source": [
    "out1 = model_own_dataset.wv.most_similar([\"summer\",\"snow\"],\"sun\")\n",
    "# out2 = model_own_dataset.wv.most_similar([\"Summer\",\"Snow\"],\"Sun\") ## Summer not present in vocab\n",
    "\n",
    "print(out1)\n",
    "# print(out2)\n",
    "\n",
    "print('\\nOutput for [\"summer\",\"snow\"],\"sun\":' + out1[0][0])\n",
    "# print('Output for [\"Summer\",\"Snow\"],\"Sun\":' + out2[0][0]) ## Summer not present in vocab\n",
    "print(\"\\n\")\n",
    "\n",
    "out1 = model_own_dataset.wv.most_similar([\"library\",\"movie\"],\"book\")\n",
    "# out2 = model_own_dataset.wv.most_similar([\"Library\",\"Movie\"],\"Book\") ## Movie not present in vocab\n",
    "\n",
    "print(out1)\n",
    "# print(out2)\n",
    "\n",
    "print('\\nOutput for [\"library\",\"movie\"],\"book\":' + out1[0][0])\n",
    "# print('Output for [\"Library\",\"Movie\"],\"Book\":' + out2[0][0]) ## Movie not present in vocab\n",
    "print(\"\\n\")\n",
    "\n",
    "# out1 = model_own_dataset.wv.most_similar([\"apple\",\"brocolli\"],\"fruits\") ## Brocolli not present in vocab\n",
    "# out2 = model_own_dataset.wv.most_similar([\"Apple\",\"Brocolli\"],\"Fruit\") ## Brocolli not present in vocab\n",
    "\n",
    "print(out1)\n",
    "# print(out2)\n",
    "\n",
    "print('\\nOutput for [\"apple\",\"brocolli\"],\"fruit\": NA')\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Conclusion for Task 2`\n",
    "\n",
    "Q: What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?\n",
    "\n",
    "> `Following points were observed`\n",
    "> 1. The pretrained model performed better in all the cases to predict the semantic similarities and encode them\n",
    "> 2. The self trained model has small vocab, hence making it difficult due to large number of unseen words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Task 3 - Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF feature extraction\n",
    "\"\"\"\n",
    "    1. reducing the size to float 32 to avoid memory issues - dtype = float32\n",
    "    2. using ngram_range to consider 1 to 4 words together while extracting the features\n",
    "\"\"\"\n",
    "tfidf = TfidfVectorizer(dtype = np.float32, min_df=2,ngram_range=(1,4))\n",
    "tfidf_vectors = tfidf.fit_transform(reviews_ratings_final['review_headline_body_processed'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "tfidf_x_train, tfidf_x_test, tfidf_y_train, tfidf_y_test = train_test_split(tfidf_vectors,\n",
    "                                                                            reviews_ratings_final[\"Class\"],\n",
    "                                                                            test_size=0.2, # Splitting the dataset in 80:20 train test split\n",
    "                                                                            random_state=34) # Setting the random state to reproduce the result\n",
    "\n",
    "## Approx run time - 32s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean vectors from word2vec pretrained model\n",
    "reviews_ratings_final['word2vec_mean'] = reviews_ratings_final['review_headline_body_processed'].apply(wv_googleNews.get_mean_vector)\n",
    "\n",
    "word2vec_mean_x_train, word2vec_mean_x_test, word2vec_mean_y_train, word2vec_mean_y_test = train_test_split(np.stack(reviews_ratings_final['word2vec_mean']), \n",
    "                                                                                                            reviews_ratings_final[\"Class\"].values,\n",
    "                                                                                                            test_size=0.2, ##Splitting the dataset in 80:20 train test split\n",
    "                                                                                                            random_state=34) # Setting the random state to reproduce the result\n",
    "\n",
    "## Approx run time - 40s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single perceptron with TFIDF and Word2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8952\n"
     ]
    }
   ],
   "source": [
    "model_perceptron_tfidf = Perceptron(random_state=34)\n",
    "model_perceptron_tfidf.fit(tfidf_x_train, tfidf_y_train)\n",
    "\n",
    "tfidf_predictions = model_perceptron_tfidf.predict(tfidf_x_test)\n",
    "eval(tfidf_y_test,tfidf_predictions)\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.825\n"
     ]
    }
   ],
   "source": [
    "model_perceptron_word2vec = Perceptron(random_state=34)\n",
    "model_perceptron_word2vec.fit(word2vec_mean_x_train,word2vec_mean_y_train)\n",
    "\n",
    "word2vec_mean_perceptron_predictions = model_perceptron_word2vec.predict(word2vec_mean_x_test)\n",
    "eval(word2vec_mean_y_test,word2vec_mean_perceptron_predictions)\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Perceptron Accuracy\n",
    "| TF_IDF Features | Word2Vec Features|\n",
    "|-----|-----|\n",
    "|89.52 %|82.5 %|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM with TF-IDF and Word2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\.virtualenvs\\Applied_NLP-VPUSIJtg\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9083\n"
     ]
    }
   ],
   "source": [
    "model_svm_tfidf = svm.LinearSVC(max_iter=50000) # setting max_iter to 50000 to avoid long runs\n",
    "model_svm_tfidf.fit(tfidf_x_train, tfidf_y_train)\n",
    "\n",
    "tfidf_svm_predictions = model_svm_tfidf.predict(tfidf_x_test)\n",
    "eval(tfidf_y_test,tfidf_svm_predictions)\n",
    "\n",
    "# Approx run time - 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\.virtualenvs\\Applied_NLP-VPUSIJtg\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8428\n"
     ]
    }
   ],
   "source": [
    "model_svm_word2vec = svm.LinearSVC(max_iter=50000) # setting max_iter to 50000 to avoid long runs\n",
    "model_svm_word2vec.fit(word2vec_mean_x_train,word2vec_mean_y_train)\n",
    "\n",
    "word2vec_mean_svm_predictions = model_svm_word2vec.predict(word2vec_mean_x_test)\n",
    "eval(word2vec_mean_y_test,word2vec_mean_svm_predictions)\n",
    "\n",
    "## Approx run time - 6s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM Accuracy\n",
    "| TF_IDF Features | Word2Vec Features|\n",
    "|-----|-----|\n",
    "|90.83  %|84.28 %|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Conclusion for Task 3 -`\n",
    "\n",
    "Q: What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and Word2Vec features)?\n",
    "> - The simple models perform better with TF-IDF features. The models are able to achieve higher accuracies on Test dataset with the TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - Feedforward Neural Networks\n",
    "\n",
    "#### Task (a) - FNN using mean vectors\n",
    "\n",
    "`Ref`\n",
    "- https://medium.com/deep-learning-study-notes/multi-layer-perceptron-mlp-in-pytorch-21ea46d50e62\n",
    "- https://stackoverflow.com/questions/60259836/cnn-indexerror-target-2-is-out-of-bounds\n",
    "- https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating two classes to iterate through the training and testing dataset extending the Dataset Class from Pytorch (https://pytorch.org/docs/stable/data.html)\n",
    "\"\"\"\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    ## The constructor assigns the X and Y labels to dataX and dataY\n",
    "    def __init__(self, XData, YData): \n",
    "        self.dataX = XData\n",
    "        self.dataY = YData\n",
    "    \n",
    "    #Returns the length of the X data\n",
    "    def __len__(self):\n",
    "        return self.dataX.shape[0]\n",
    "    \n",
    "    # Returns X and Y data at given index\n",
    "    def __getitem__(self, index):\n",
    "        x = self.dataX[index]\n",
    "        y = self.dataY[index]\n",
    "        return x, y\n",
    "\n",
    "class TestDataset(TrainDataset):\n",
    "    # The Test dataset contains just the X label\n",
    "    def __getitem__(self, index):\n",
    "        x = self.dataX[index]\n",
    "        return x\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating data loaders from the datasets with word2vec mean vectors (https://pytorch.org/docs/stable/data.html)\n",
    "\"\"\"\n",
    "\n",
    "train_set = TrainDataset(word2vec_mean_x_train, word2vec_mean_y_train)\n",
    "test_set  = TestDataset(word2vec_mean_x_test, word2vec_mean_y_test)\n",
    "\n",
    "## Setting the batch size\n",
    "### Trained the model with different batch sizes and 64 performs the best on test dataset\n",
    "batch_size = 64 \n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the model architecture for this task\n",
    "\n",
    "- (0): Linear - in_features=300, out_features=50\n",
    "- (1): ReLU - ReLU layer\n",
    "- (2): Dropout - Dropping 20% of the paths to avoid overfitting\n",
    "- (3): Linear - in_features=50, out_features=5\n",
    "- (4): LeakyReLU - LeakyRelu Layer\n",
    "- (5): Linear - in_features=5, out_features=2\n",
    "- (6): LeakyReLU - LeakyRelu Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN_MLP(\n",
      "  (sequntial): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=50, out_features=5, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): Linear(in_features=5, out_features=2, bias=True)\n",
      "    (6): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FFN_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFN_MLP, self).__init__()\n",
    "        self.sequntial = nn.Sequential(nn.Linear(300, 50), \n",
    "                                       nn.ReLU(), \n",
    "                                       nn.Dropout(0.2),\n",
    "                                       nn.Linear(50,5),\n",
    "                                       nn.LeakyReLU(),\n",
    "                                       # nn.Dropout(0.2),\n",
    "                                       nn.Linear(5,2),\n",
    "                                       nn.LeakyReLU())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.sequntial(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model_FNN = FFN_MLP().to(device)\n",
    "optimizer_FNN = torch.optim.Adam(model_FNN.parameters(),lr=0.001)\n",
    "criterion_FNN = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model_FNN)\n",
    "\n",
    "## Approx run time - 2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the FNN, Running 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss = 0.44\n",
      "Epoch 2 - Average Loss = 0.36\n",
      "Epoch 3 - Average Loss = 0.35\n",
      "Epoch 4 - Average Loss = 0.34\n",
      "Epoch 5 - Average Loss = 0.33\n",
      "Epoch 6 - Average Loss = 0.33\n",
      "Epoch 7 - Average Loss = 0.32\n",
      "Epoch 8 - Average Loss = 0.32\n",
      "Epoch 9 - Average Loss = 0.32\n",
      "Epoch 10 - Average Loss = 0.31\n",
      "Epoch 11 - Average Loss = 0.31\n",
      "Epoch 12 - Average Loss = 0.31\n",
      "Epoch 13 - Average Loss = 0.30\n",
      "Epoch 14 - Average Loss = 0.30\n",
      "Epoch 15 - Average Loss = 0.30\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "# Training the model\n",
    "model_FNN.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader):\n",
    "        optimizer_FNN.zero_grad()\n",
    "\n",
    "        #Reading the data from train_loader\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Generating the predictions (forward pass)\n",
    "        output = model_FNN(x).to(device)\n",
    "\n",
    "        # Calculating the losses and performing backward pass\n",
    "        loss = criterion_FNN(output, y)\n",
    "        loss.backward()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        optimizer_FNN.step()\n",
    "\n",
    "    print('Epoch ' + str(epoch + 1)+ ' - Average Loss = ' + '{:.2f}'.format(np.average(losses))) # Prints the average loss of all the batches in the current epoch\n",
    "\n",
    "## Approx run time for 15 epochs - 2min 15s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8648\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "model_FNN.eval()\n",
    "output = torch.tensor([]).to(device)\n",
    "for x_label in test_loader:\n",
    "\n",
    "    # Loading the data from test loader\n",
    "    x = x_label.to(device)\n",
    "    \n",
    "    output = torch.cat((output,model_FNN(x).to(device)))\n",
    "\n",
    "eval(word2vec_mean_y_test,output.argmax(dim=1).to('cpu'))\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FNN with mean features Accuracy\n",
    "\n",
    "Q: Report accuracy values on the testing split for your MLP\n",
    "| Accuracy|\n",
    "|-----|\n",
    "|86.48 %|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task (b) - FNN using first 10 word2vec vectors\n",
    "\n",
    "`Ref` - \n",
    "- https://stackoverflow.com/questions/72480289/how-to-handle-keyerrorfkey-key-not-present-wor2vec-with-gensim\n",
    "- https://stackoverflow.com/questions/65372032/deal-with-out-of-vocabulary-word-with-gensim-pretrained-glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function returns a vector of size 3000 with the word2vec features of 10 words\n",
    "- The function take processed tokenized string list as input\n",
    "- It considers the first 10 words which are present in the dataset and skips the words which are not present in vocab of Google news word2vec model (https://piazza.com/class/llm91seaknw3j6/post/333_f2)\n",
    "- If the length of known words is less than 10, then it is padded with 0s to make the final size of the output to be 3000\n",
    "\"\"\"\n",
    "\n",
    "def get_word2vec_first_10(s:list):\n",
    "    vector_first_10 = []\n",
    "    i = 0\n",
    "    iterator = 0\n",
    "    \n",
    "    while i < 10 and iterator < len(s):\n",
    "        try:\n",
    "            current_vec = wv_googleNews.get_vector(s[iterator])\n",
    "            vector_first_10 = np.concatenate((vector_first_10, current_vec))\n",
    "        except:\n",
    "            i-=1 ## discards the pass if the word is out of vocubulary\n",
    "        finally:\n",
    "            i+=1\n",
    "            iterator+=1\n",
    "    \n",
    "    \n",
    "    vector_first_10 = np.pad(vector_first_10,(0, 3000 - len(vector_first_10))) ## Padding the final output with 0 in order to generate a 3000 shape vector\n",
    "    return vector_first_10\n",
    "\n",
    "## Approx run time - 1 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the above function and saving the vector in word2vec_first_10 series\n",
    "reviews_ratings_final['word2vec_first_10'] = reviews_ratings_final['review_headline_body_processed'].apply(get_word2vec_first_10)\n",
    "\n",
    "## Approx run time - 20s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset in 80:20 train:test split\n",
    "word2vec_first_10_x_train, word2vec_first_10_x_test, word2vec_first_10_y_train, word2vec_first_10_y_test = train_test_split(np.stack(reviews_ratings_final['word2vec_first_10'].values), \n",
    "                                                                                                                            reviews_ratings_final[\"Class\"].values,\n",
    "                                                                                                                            test_size=0.2,random_state=34) # Setting the random state to reproduce the result\n",
    "\n",
    "## Approx run time - 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating data loaders from the datasets with word2vec mean vectors (https://pytorch.org/docs/stable/data.html)\n",
    "\"\"\"\n",
    "\n",
    "train_set_first_10 = TrainDataset(word2vec_first_10_x_train, word2vec_first_10_y_train)\n",
    "test_set_first_10  = TestDataset(word2vec_first_10_x_test, word2vec_first_10_y_test)\n",
    "\n",
    "\n",
    "## Setting the batch size\n",
    "### Trained the model with different batch sizes and 128 performs the best on test dataset\n",
    "batch_size = 128\n",
    "train_loader_first_10 = DataLoader(train_set_first_10, batch_size=batch_size, shuffle=True)\n",
    "test_loader_first_10  = DataLoader(test_set_first_10,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the model architecture for this task\n",
    "\n",
    "- (0): Linear - in_features=3000, out_features=50\n",
    "- (1): ReLU - ReLU layer\n",
    "- (2): Dropout - Dropping 20% of the paths to avoid overfitting\n",
    "- (3): Linear - in_features=50, out_features=5\n",
    "- (4): LeakyReLU - LeakyRelu Layer\n",
    "- (5):Dropout - Dropping 20% of the paths to avoid overfitting\n",
    "- (6): Linear - in_features=5, out_features=2\n",
    "- (7): LeakyReLU - LeakyRelu Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN_first_10(\n",
      "  (sequntial): Sequential(\n",
      "    (0): Linear(in_features=3000, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=50, out_features=5, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=5, out_features=2, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FFN_first_10(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size1,hidden_size2,output_size):\n",
    "        super(FFN_first_10, self).__init__()\n",
    "\n",
    "        self.sequntial = nn.Sequential(nn.Linear(input_size, hidden_size1),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(0.2),\n",
    "                                       nn.Linear(hidden_size1,hidden_size2),\n",
    "                                       nn.LeakyReLU(),\n",
    "                                       nn.Dropout(0.2),\n",
    "                                       nn.Linear(hidden_size2,output_size),\n",
    "                                       nn.LeakyReLU())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.sequntial(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "input_size = 3000\n",
    "hidden_size1 = 50\n",
    "hidden_size2 = 5\n",
    "output_size = 2\n",
    "\n",
    "## Creating model\n",
    "model_first_10_FNN = FFN_first_10(input_size,hidden_size1,hidden_size2,output_size).to(device)\n",
    "optimizer_first_10_FNN = torch.optim.Adam(model_first_10_FNN.parameters(),lr=0.001)\n",
    "criterion_first_10_FNN = nn.CrossEntropyLoss()\n",
    "print(model_first_10_FNN)\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the FNN with first 10 vec features, Running 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss = 0.45\n",
      "Epoch 2 - Average Loss = 0.37\n",
      "Epoch 3 - Average Loss = 0.33\n",
      "Epoch 4 - Average Loss = 0.31\n",
      "Epoch 5 - Average Loss = 0.29\n",
      "Epoch 6 - Average Loss = 0.27\n",
      "Epoch 7 - Average Loss = 0.25\n",
      "Epoch 8 - Average Loss = 0.23\n",
      "Epoch 9 - Average Loss = 0.21\n",
      "Epoch 10 - Average Loss = 0.20\n",
      "Epoch 11 - Average Loss = 0.19\n",
      "Epoch 12 - Average Loss = 0.17\n",
      "Epoch 13 - Average Loss = 0.16\n",
      "Epoch 14 - Average Loss = 0.16\n",
      "Epoch 15 - Average Loss = 0.15\n",
      "Epoch 16 - Average Loss = 0.14\n",
      "Epoch 17 - Average Loss = 0.13\n",
      "Epoch 18 - Average Loss = 0.13\n",
      "Epoch 19 - Average Loss = 0.13\n",
      "Epoch 20 - Average Loss = 0.12\n",
      "Epoch 21 - Average Loss = 0.11\n",
      "Epoch 22 - Average Loss = 0.11\n",
      "Epoch 23 - Average Loss = 0.11\n",
      "Epoch 24 - Average Loss = 0.11\n",
      "Epoch 25 - Average Loss = 0.11\n",
      "Epoch 26 - Average Loss = 0.10\n",
      "Epoch 27 - Average Loss = 0.10\n",
      "Epoch 28 - Average Loss = 0.09\n",
      "Epoch 29 - Average Loss = 0.09\n",
      "Epoch 30 - Average Loss = 0.09\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "# Training a model\n",
    "model_first_10_FNN.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader_first_10):\n",
    "        optimizer_first_10_FNN.zero_grad()\n",
    "\n",
    "        #Reading the data from train_loader\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Generating the predictions (forward pass)\n",
    "        output = model_first_10_FNN(x)\n",
    "                \n",
    "        # Calculating the losses and performing backward pass\n",
    "        loss = criterion_first_10_FNN(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer_first_10_FNN.step()\n",
    "\n",
    "    print('Epoch ' + str(epoch + 1)+ ' - Average Loss = ' + '{:.2f}'.format(np.average(losses))) # Prints the average loss of all the batches in the current epoch\n",
    "\n",
    "# Run time for 30 epochs 2m 45s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8213\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "\n",
    "output_first_10_FNN = torch.tensor([]).to(device)\n",
    "for x_label in test_loader_first_10:\n",
    "    x = x_label.to(device).float()\n",
    "    \n",
    "    output_first_10_FNN = torch.cat((output_first_10_FNN,model_first_10_FNN(x.to(device)).to(device).argmax(dim=1)))\n",
    "\n",
    "eval(word2vec_mean_y_test,output_first_10_FNN.to('cpu'))\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FNN with first 10 Word2vec vectors Accuracy\n",
    "\n",
    "Q: Report the accuracy value on the testing split for your MLP model\n",
    "\n",
    "| Accuracy|\n",
    "|-----|\n",
    "|82.13 %|\n",
    "\n",
    "Q: What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section.\n",
    "\n",
    "> Using word2vec features, the performance of all the models is in the following order -\n",
    "> - FNN with Mean Vectors\n",
    "> - SVM\n",
    "> - Single Perceptron\n",
    "> - FNN with First 10 vectors \\\n",
    "> The performance is comparable in all the models ranging from 82 to 86 % on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Recurrent Neural Networks\n",
    "\n",
    "#### Task (a) - Simple RNN\n",
    "\n",
    "`Ref` - \n",
    " - https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the model architecture for this task\n",
    "\n",
    "- (0): Recurrent Neural Network layer - in_features=3000, out_features=10\n",
    "- (1): Linear - in_features=10, out_features=2\n",
    "- (2): ReLU - Relu Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_first_10(\n",
      "  (rnn): RNN(3000, 10, batch_first=True)\n",
      "  (lin): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (act2): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNN_first_10(nn.Module):\n",
    "    def __init__(self, input_size:int, hidden_size:int, output_size:int):\n",
    "        super(RNN_first_10, self).__init__()\n",
    "         \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)    ## RNN layer with input size 3000 and output size of 10\n",
    "        # self.dropout = nn.Dropout(0.2) ## Tried drop out layer, but performs better without it on test dataset\n",
    "        self.lin =  nn.Linear(hidden_size, output_size) ## Linear layer with input size of 10 and output size of 2\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,_ = self.rnn(x)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.lin(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "input_size = 3000 \n",
    "hidden_size = 10\n",
    "output_size = 2\n",
    "\n",
    "# Creating the model\n",
    "model_rnn = RNN_first_10(input_size, hidden_size, output_size).to(device)\n",
    "criterion_rnn = nn.CrossEntropyLoss()\n",
    "optimizer_rnn = torch.optim.Adam(model_rnn.parameters(),lr=0.0001)\n",
    "print(model_rnn)\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the RNN, Running 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss = 0.66\n",
      "Epoch 2 - Average Loss = 0.58\n",
      "Epoch 3 - Average Loss = 0.51\n",
      "Epoch 4 - Average Loss = 0.47\n",
      "Epoch 5 - Average Loss = 0.45\n",
      "Epoch 6 - Average Loss = 0.43\n",
      "Epoch 7 - Average Loss = 0.42\n",
      "Epoch 8 - Average Loss = 0.41\n",
      "Epoch 9 - Average Loss = 0.40\n",
      "Epoch 10 - Average Loss = 0.39\n",
      "Epoch 11 - Average Loss = 0.38\n",
      "Epoch 12 - Average Loss = 0.38\n",
      "Epoch 13 - Average Loss = 0.37\n",
      "Epoch 14 - Average Loss = 0.37\n",
      "Epoch 15 - Average Loss = 0.36\n",
      "Epoch 16 - Average Loss = 0.36\n",
      "Epoch 17 - Average Loss = 0.36\n",
      "Epoch 18 - Average Loss = 0.35\n",
      "Epoch 19 - Average Loss = 0.35\n",
      "Epoch 20 - Average Loss = 0.34\n",
      "Epoch 21 - Average Loss = 0.34\n",
      "Epoch 22 - Average Loss = 0.34\n",
      "Epoch 23 - Average Loss = 0.34\n",
      "Epoch 24 - Average Loss = 0.33\n",
      "Epoch 25 - Average Loss = 0.33\n",
      "Epoch 26 - Average Loss = 0.33\n",
      "Epoch 27 - Average Loss = 0.33\n",
      "Epoch 28 - Average Loss = 0.32\n",
      "Epoch 29 - Average Loss = 0.32\n",
      "Epoch 30 - Average Loss = 0.32\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "#Training the model\n",
    "model_rnn.train()\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader_first_10):\n",
    "        optimizer_rnn.zero_grad()\n",
    "\n",
    "        ## Loading the data from train loader\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        ## Predicting the labels (Forward pass)\n",
    "        output = model_rnn(x)\n",
    "        \n",
    "        ## Calculating the loss and performing the backward pass\n",
    "        loss = criterion_rnn(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer_rnn.step()\n",
    "\n",
    "    print('Epoch ' + str(epoch + 1)+ ' - Average Loss = ' + '{:.2f}'.format(np.average(losses)))\n",
    "\n",
    "# Run time for 30 epochs 2m 45s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8319\n"
     ]
    }
   ],
   "source": [
    "output_first_10 = torch.tensor([]).to(device)\n",
    "for x_label in test_loader_first_10:\n",
    "    x = x_label.to(device).float()\n",
    "    \n",
    "    output_first_10 = torch.cat((output_first_10,model_rnn(x.to(device)).to(device).argmax(dim=1)))\n",
    "\n",
    "eval(word2vec_first_10_y_test,output_first_10.to('cpu'))\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple RNN with Word2vec features Accuracy\n",
    "\n",
    "Q: Report accuracy values on the testing split for your RNN model.\n",
    "| Accuracy|\n",
    "|-----|\n",
    "|83.19 %|\n",
    "\n",
    "Q: What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models?\n",
    "#### `Conclusion` \n",
    "> For the current dataset- \n",
    "> - The Feedforward network model with mean vector (Task 4a) performs better than the Simple RNN model\n",
    "> - However, Simple RNN performs better than the FNN with first 10 word2vec vectors(Task 4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task (b) - Gated RNN\n",
    "\n",
    "`Ref` - \n",
    "- https://blog.floydhub.com/gru-with-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the model architecture for this task\n",
    "\n",
    "- (0): Gated Recurrent Neural Network layer - in_features=3000, out_features=10\n",
    "- (1): Linear - in_features=10, out_features=2\n",
    "- (2): ReLU - Relu Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_first_10_gated(\n",
      "  (rnn): GRU(3000, 10, batch_first=True)\n",
      "  (lin): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (act1): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNN_first_10_gated(nn.Module):\n",
    "    def __init__(self, input_size:int, hidden_size:int, output_size:int):\n",
    "        super(RNN_first_10_gated, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True) ## Gated RNN layer with input size 3000 and output size of 10\n",
    "        self.lin =  nn.Linear(hidden_size, output_size) ## Linear layer with input size of 10 and output size of 2\n",
    "        self.act1 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,_ = self.rnn(x)\n",
    "        out = self.lin(out)\n",
    "        out = self.act1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "input_size = 3000\n",
    "hidden_size = 10\n",
    "output_size = 2\n",
    "\n",
    "# Creating the models\n",
    "model_gru = RNN_first_10_gated(input_size, hidden_size, output_size).to(device)\n",
    "criterion_gru = nn.CrossEntropyLoss()\n",
    "optimizer_gru = torch.optim.Adam(model_gru.parameters(), lr=0.0001)\n",
    "print(model_gru)\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Gated RNN, Running 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss = 0.67\n",
      "Epoch 2 - Average Loss = 0.59\n",
      "Epoch 3 - Average Loss = 0.51\n",
      "Epoch 4 - Average Loss = 0.47\n",
      "Epoch 5 - Average Loss = 0.44\n",
      "Epoch 6 - Average Loss = 0.42\n",
      "Epoch 7 - Average Loss = 0.41\n",
      "Epoch 8 - Average Loss = 0.40\n",
      "Epoch 9 - Average Loss = 0.39\n",
      "Epoch 10 - Average Loss = 0.38\n",
      "Epoch 11 - Average Loss = 0.38\n",
      "Epoch 12 - Average Loss = 0.37\n",
      "Epoch 13 - Average Loss = 0.37\n",
      "Epoch 14 - Average Loss = 0.36\n",
      "Epoch 15 - Average Loss = 0.36\n",
      "Epoch 16 - Average Loss = 0.35\n",
      "Epoch 17 - Average Loss = 0.35\n",
      "Epoch 18 - Average Loss = 0.35\n",
      "Epoch 19 - Average Loss = 0.34\n",
      "Epoch 20 - Average Loss = 0.34\n",
      "Epoch 21 - Average Loss = 0.34\n",
      "Epoch 22 - Average Loss = 0.33\n",
      "Epoch 23 - Average Loss = 0.33\n",
      "Epoch 24 - Average Loss = 0.33\n",
      "Epoch 25 - Average Loss = 0.32\n",
      "Epoch 26 - Average Loss = 0.32\n",
      "Epoch 27 - Average Loss = 0.32\n",
      "Epoch 28 - Average Loss = 0.32\n",
      "Epoch 29 - Average Loss = 0.31\n",
      "Epoch 30 - Average Loss = 0.31\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader_first_10):\n",
    "        optimizer_gru.zero_grad()\n",
    "\n",
    "        ## Loading the data from train loader\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        ## Predicting the labels (Forward pass)\n",
    "        output = model_gru(x)\n",
    "        \n",
    "        ## Calculating the loss and performing the backward pass\n",
    "        loss = criterion_gru(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer_gru.step()\n",
    "    \n",
    "    print('Epoch ' + str(epoch + 1)+ ' - Average Loss = ' + '{:.2f}'.format(np.average(losses)))\n",
    "\n",
    "# Run time for 30 epochs 2m 45s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8369\n"
     ]
    }
   ],
   "source": [
    "output_first_10 = torch.tensor([]).to(device)\n",
    "for x_label in test_loader_first_10:\n",
    "    x = x_label.to(device).float()\n",
    "    \n",
    "    output_first_10 = torch.cat((output_first_10,model_gru(x.to(device)).to(device).argmax(dim=1)))\n",
    "\n",
    "eval(word2vec_first_10_y_test,output_first_10.to('cpu'))\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gated RNN with Word2vec features Accuracy\n",
    "| Accuracy|\n",
    "|-----|\n",
    "|83.69 %|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task (c) - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the model architecture for this task\n",
    "\n",
    "- (0): LSTM Neural Network layer - in_features=3000, out_features=10\n",
    "- (1): Linear - in_features=10, out_features=2\n",
    "- (2): ReLU - Relu Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_first_10(\n",
      "  (lstm): LSTM(3000, 10, batch_first=True)\n",
      "  (lin): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (act): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LSTM_first_10(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM_first_10, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True) ## LSTM layer with input size 3000 and output size 10\n",
    "        self.lin =  nn.Linear(hidden_size, output_size) ## Linear layer with input size 10 and output size 2\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,_ = self.lstm(x)\n",
    "        out = self.lin(out)\n",
    "        out = self.act(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "input_size = 3000\n",
    "hidden_size = 10\n",
    "output_size = 2\n",
    "\n",
    "## Creating the model\n",
    "model_lstm = LSTM_first_10(input_size, hidden_size, output_size).to(device)\n",
    "criterion_lstm = nn.CrossEntropyLoss()\n",
    "optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "print(model_lstm)\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model, Running 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss = 0.47\n",
      "Epoch 2 - Average Loss = 0.36\n",
      "Epoch 3 - Average Loss = 0.32\n",
      "Epoch 4 - Average Loss = 0.30\n",
      "Epoch 5 - Average Loss = 0.27\n",
      "Epoch 6 - Average Loss = 0.25\n",
      "Epoch 7 - Average Loss = 0.23\n",
      "Epoch 8 - Average Loss = 0.21\n",
      "Epoch 9 - Average Loss = 0.19\n",
      "Epoch 10 - Average Loss = 0.17\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader_first_10):\n",
    "        optimizer_lstm.zero_grad()\n",
    "\n",
    "        ## Loading the data from train loader\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        ## Predicting the labels (Forward pass)\n",
    "        output = model_lstm(x)\n",
    "\n",
    "        ## Calculating the loss and performing the backward pass\n",
    "        loss = criterion_lstm(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer_lstm.step()\n",
    "\n",
    "    print('Epoch ' + str(epoch + 1)+ ' - Average Loss = ' + '{:.2f}'.format(np.average(losses)))\n",
    "\n",
    "# Run time for 10 epochs 1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8327\n"
     ]
    }
   ],
   "source": [
    "output_first_10 = torch.tensor([]).to(device)\n",
    "for x_label in test_loader_first_10:\n",
    "    x = x_label.to(device).float()\n",
    "    \n",
    "    output_first_10 = torch.cat((output_first_10,model_lstm(x.to(device)).to(device).argmax(dim=1)))\n",
    "\n",
    "eval(word2vec_first_10_y_test,output_first_10.to('cpu'))\n",
    "\n",
    "## Approx run time - 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM with Word2vec features Accuracy\n",
    "\n",
    "Q: Report accuracy values on the testing split for your LSTM model.\n",
    "| Accuracy|\n",
    "|-----|\n",
    "|83.27 %|\n",
    "\n",
    "\n",
    "Q: What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN?\n",
    "> - The accuracies are comparable for all the three models, however Gated Recurrent Neural net has the highest of all on the test dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Conclusion` - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Accuracy Values as of 10/19\n",
    "\n",
    "| Model | Input Features |Accuracy |\n",
    "| ----- | ----- | ----- |\n",
    "| Perceptron | Mean Word2Vec | 82.5 %|\n",
    "|  | TF_IDF |89.52 % |\n",
    "| SVM | Mean Word2Vec |84.28 %|\n",
    "|  | TF_IDF |90.83 % |\n",
    "| FNN | Mean Word2Vec | 86.48 %|\n",
    "| FNN | First 10 Word2Vec | 82.13 %|\n",
    "| Simple RNN | First 10 Word2Vec |83.19 %|\n",
    "| Gated RNN | First 10 Word2Vec |83.69 %|\n",
    "| LSTM | First 10 Word2Vec |83.27 %|\n",
    "\n",
    "The highest performance on the unseen test dataset is given by SVM model with TF_IDF features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
