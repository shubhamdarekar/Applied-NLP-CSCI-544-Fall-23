{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(actual,predicted):\n",
    "    prf = precision_recall_fscore_support(actual,predicted, average='binary')\n",
    "    print(str(round(prf[0],4))+\" \"+str(round(prf[1],4))+\" \"+str(round(prf[2],4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews_ratings = pd.read_csv(\"D:\\\\Applied NLP\\\\HW1\\\\amazon_reviews_us_Office_Products_v1_00.tsv\", sep='\\t', engine=\"python\", quoting=3)\n",
    "reviews_ratings = reviews_ratings[['review_headline','review_body','star_rating']]\n",
    "\n",
    "## Filling Na values with blank as one of the column from headline and body might contain useful data\n",
    "reviews_ratings['review_headline'].fillna(\"\", inplace=True) \n",
    "reviews_ratings['review_body'].fillna(\"\", inplace=True)\n",
    "\n",
    "\n",
    "reviews_ratings['review_headline_body'] = reviews_ratings['review_headline'] + \" \" + reviews_ratings['review_body']\n",
    "\n",
    "#Random selection of 50,000 rows from each class\n",
    "df_class1 = reviews_ratings[reviews_ratings['star_rating']>3].sample(n=50000, random_state=34) # setting the random state to reporduce the output\n",
    "df_class1['Class'] = 1\n",
    "df_class2 = reviews_ratings[reviews_ratings['star_rating']<=3].sample(n=50000, random_state=34) # setting the random state to reporduce the output\n",
    "df_class2['Class'] = 0\n",
    "\n",
    "reviews_ratings_final = pd.concat([df_class1, df_class2], ignore_index=True, sort=False).reset_index(drop=True) #concating the classes\n",
    "\n",
    "## Approx run time - 1min 20s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Word Embeddings\n",
    "\n",
    "#### Task a) \n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "# print(wv.similarity('king','queen'))\n",
    "# print(wv.similarity('man','woman'))\n",
    "# print(wv.similarity('excellent','outstanding'))\n",
    "\n",
    "## Approx run time - 1min 30s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model_own_dataset = Word2Vec(sentences=reviews_ratings_final['review_headline_body'].apply(lambda x: str(x).split()),vector_size=300,window=13,min_count=9)\n",
    "\n",
    "## Approx run time - 1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7228375\n",
      "0.50202453\n"
     ]
    }
   ],
   "source": [
    "# print(model.wv.similarity('king','queen'))\n",
    "print(model_own_dataset.wv.similarity('man','woman'))\n",
    "print(model_own_dataset.wv.similarity('good','excellent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\"\"\"\n",
    "    1. reducing the size to float 32 to avoid memory issues - dtype = float32\n",
    "    2. using ngram_range to consider 1 to 4 words together while extracting the features\n",
    "\"\"\"\n",
    "tfidf = TfidfVectorizer(dtype = np.float32, min_df=2,ngram_range=(1,4))\n",
    "tfidf_vectors = tfidf.fit_transform(reviews_ratings_final['review_headline_body'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf_x_train, tfidf_x_test, tfidf_y_train, tfidf_y_test = train_test_split(tfidf_vectors, reviews_ratings_final[\"Class\"],test_size=0.2,random_state=34) # Setting the random state to reproduce the result\n",
    "\n",
    "## Approx run time - 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean vectors from word2vec pretrained model\n",
    "reviews_ratings_final['word2vec_mean'] = reviews_ratings_final['review_headline_body'].apply(lambda x: wv.get_mean_vector(str(x).split()))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "word2vec_mean_x_train, word2vec_mean_x_test, word2vec_mean_y_train, word2vec_mean_y_test = train_test_split(np.stack(reviews_ratings_final['word2vec_mean']), reviews_ratings_final[\"Class\"].values,test_size=0.2,random_state=34) # Setting the random state to reproduce the result\n",
    "\n",
    "## Approx run time - 2 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7312 0.9558 0.8285\n"
     ]
    }
   ],
   "source": [
    "model_perceptron_word2vec = Perceptron(random_state=34)\n",
    "model_perceptron_word2vec.fit(word2vec_mean_x_train,word2vec_mean_y_train.values)\n",
    "\n",
    "word2vec_mean_perceptron_predictions = model_perceptron_word2vec.predict(word2vec_mean_x_test)\n",
    "eval(word2vec_mean_y_test,word2vec_mean_perceptron_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9104 0.9166 0.9135\n"
     ]
    }
   ],
   "source": [
    "model_perceptron_tfidf = Perceptron(random_state=34)\n",
    "model_perceptron_tfidf.fit(tfidf_x_train, tfidf_y_train)\n",
    "\n",
    "tfidf_predictions = model_perceptron_tfidf.predict(tfidf_x_test)\n",
    "eval(tfidf_y_test,tfidf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shubh\\.virtualenvs\\Applied_NLP-VPUSIJtg\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.879 0.0245 0.0476\n"
     ]
    }
   ],
   "source": [
    "model_svm_word2vec = svm.SVC(kernel='linear', max_iter=10000) # setting max_iter to 10000 to avoid long runs\n",
    "model_svm_word2vec.fit(word2vec_mean_x_train,word2vec_mean_y_train.values)\n",
    "\n",
    "word2vec_mean_svm_predictions = model_svm_word2vec.predict(word2vec_mean_x_test)\n",
    "eval(word2vec_mean_y_test,word2vec_mean_svm_predictions)\n",
    "\n",
    "## Approx run time - 65 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shubh\\.virtualenvs\\Applied_NLP-VPUSIJtg\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 1188908 features, but SVC is expecting 300 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Applied NLP\\HW3\\HW3.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Applied%20NLP/HW3/HW3.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_svm_tfidf \u001b[39m=\u001b[39m svm\u001b[39m.\u001b[39mSVC(kernel\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m, max_iter\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m) \u001b[39m# setting max_iter to 10000 to avoid long runs\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Applied%20NLP/HW3/HW3.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_svm_tfidf\u001b[39m.\u001b[39mfit(tfidf_x_train, tfidf_y_train)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Applied%20NLP/HW3/HW3.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tfidf_svm_predictions \u001b[39m=\u001b[39m model_svm_word2vec\u001b[39m.\u001b[39;49mpredict(tfidf_x_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Applied%20NLP/HW3/HW3.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39meval\u001b[39m(tfidf_y_test,tfidf_svm_predictions)\n",
      "File \u001b[1;32mc:\\Users\\shubh\\.virtualenvs\\Applied_NLP-VPUSIJtg\\lib\\site-packages\\sklearn\\svm\\_base.py:818\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    816\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_function(X), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    817\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 818\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m    819\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp))\n",
      "File \u001b[1;32mc:\\Users\\shubh\\.virtualenvs\\Applied_NLP-VPUSIJtg\\lib\\site-packages\\sklearn\\svm\\_base.py:431\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    416\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \n\u001b[0;32m    418\u001b[0m \u001b[39m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39m        The predicted values.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 431\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_for_predict(X)\n\u001b[0;32m    432\u001b[0m     predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_predict \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dense_predict\n\u001b[0;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32mc:\\Users\\shubh\\.virtualenvs\\Applied_NLP-VPUSIJtg\\lib\\site-packages\\sklearn\\svm\\_base.py:611\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    608\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    610\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel):\n\u001b[1;32m--> 611\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    612\u001b[0m         X,\n\u001b[0;32m    613\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    614\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[0;32m    615\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    616\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    617\u001b[0m         reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sp\u001b[39m.\u001b[39missparse(X):\n\u001b[0;32m    621\u001b[0m     X \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32mc:\\Users\\shubh\\.virtualenvs\\Applied_NLP-VPUSIJtg\\lib\\site-packages\\sklearn\\base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 626\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    628\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\shubh\\.virtualenvs\\Applied_NLP-VPUSIJtg\\lib\\site-packages\\sklearn\\base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 415\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1188908 features, but SVC is expecting 300 features as input."
     ]
    }
   ],
   "source": [
    "model_svm_tfidf = svm.SVC(kernel='linear', max_iter=10000) # setting max_iter to 10000 to avoid long runs\n",
    "model_svm_tfidf.fit(tfidf_x_train, tfidf_y_train)\n",
    "\n",
    "tfidf_svm_predictions = model_svm_tfidf.predict(tfidf_x_test)\n",
    "eval(tfidf_y_test,tfidf_svm_predictions)\n",
    "\n",
    "## Approx run time - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9194 0.9246 0.922\n"
     ]
    }
   ],
   "source": [
    "tfidf_svm_predictions = model_svm_tfidf.predict(tfidf_x_test)\n",
    "eval(tfidf_y_test,tfidf_svm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 A - FFN \n",
    "\n",
    "\n",
    "https://medium.com/deep-learning-study-notes/multi-layer-perceptron-mlp-in-pytorch-21ea46d50e62\n",
    "https://stackoverflow.com/questions/60259836/cnn-indexerror-target-2-is-out-of-bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class FFN_perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFN_perceptron, self).__init__()\n",
    "        self.sequntial = nn.Sequential(nn.Linear(300, 50), nn.ReLU(), nn.Linear(50,5), nn.ReLU(), nn.Linear(5,3))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.sequntial(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = FFN_perceptron().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, XData, YData):\n",
    "        self.dataX = XData\n",
    "        self.dataY = YData\n",
    "    def __len__(self):\n",
    "        return self.dataX.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        x = self.dataX[index]\n",
    "        y = self.dataY[index]\n",
    "        return x, y\n",
    "\n",
    "class TestDataset(TrainDataset):\n",
    "    def __getitem__(self, index):\n",
    "        x = self.dataX[index]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec_mean_x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Applied NLP\\HW3\\HW3.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Applied%20NLP/HW3/HW3.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_set \u001b[39m=\u001b[39m TrainDataset(word2vec_mean_x_train, word2vec_mean_y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Applied%20NLP/HW3/HW3.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_set  \u001b[39m=\u001b[39m TestDataset(word2vec_mean_x_test, word2vec_mean_y_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Applied%20NLP/HW3/HW3.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word2vec_mean_x_train' is not defined"
     ]
    }
   ],
   "source": [
    "train_set = TrainDataset(word2vec_mean_x_train, word2vec_mean_y_train)\n",
    "test_set  = TestDataset(word2vec_mean_x_test, word2vec_mean_y_test)\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 | Batch 0 | Loss   1.17\n",
      "\tEpoch 0 | Batch 40 | Loss   1.10\n",
      "\tEpoch 0 | Batch 80 | Loss   0.95\n",
      "\tEpoch 0 | Batch 120 | Loss   0.72\n",
      "Epoch 0 | Loss   0.92\n",
      "\tEpoch 1 | Batch 0 | Loss   0.59\n",
      "\tEpoch 1 | Batch 40 | Loss   0.53\n",
      "\tEpoch 1 | Batch 80 | Loss   0.46\n",
      "\tEpoch 1 | Batch 120 | Loss   0.40\n",
      "Epoch 1 | Loss   0.49\n",
      "\tEpoch 2 | Batch 0 | Loss   0.43\n",
      "\tEpoch 2 | Batch 40 | Loss   0.44\n",
      "\tEpoch 2 | Batch 80 | Loss   0.41\n",
      "\tEpoch 2 | Batch 120 | Loss   0.42\n",
      "Epoch 2 | Loss   0.42\n",
      "\tEpoch 3 | Batch 0 | Loss   0.44\n",
      "\tEpoch 3 | Batch 40 | Loss   0.43\n",
      "\tEpoch 3 | Batch 80 | Loss   0.40\n",
      "\tEpoch 3 | Batch 120 | Loss   0.39\n",
      "Epoch 3 | Loss   0.40\n",
      "\tEpoch 4 | Batch 0 | Loss   0.40\n",
      "\tEpoch 4 | Batch 40 | Loss   0.37\n",
      "\tEpoch 4 | Batch 80 | Loss   0.41\n",
      "\tEpoch 4 | Batch 120 | Loss   0.37\n",
      "Epoch 4 | Loss   0.39\n",
      "\tEpoch 5 | Batch 0 | Loss   0.38\n",
      "\tEpoch 5 | Batch 40 | Loss   0.39\n",
      "\tEpoch 5 | Batch 80 | Loss   0.40\n",
      "\tEpoch 5 | Batch 120 | Loss   0.34\n",
      "Epoch 5 | Loss   0.38\n",
      "\tEpoch 6 | Batch 0 | Loss   0.40\n",
      "\tEpoch 6 | Batch 40 | Loss   0.34\n",
      "\tEpoch 6 | Batch 80 | Loss   0.40\n",
      "\tEpoch 6 | Batch 120 | Loss   0.35\n",
      "Epoch 6 | Loss   0.37\n",
      "\tEpoch 7 | Batch 0 | Loss   0.37\n",
      "\tEpoch 7 | Batch 40 | Loss   0.34\n",
      "\tEpoch 7 | Batch 80 | Loss   0.33\n",
      "\tEpoch 7 | Batch 120 | Loss   0.40\n",
      "Epoch 7 | Loss   0.37\n",
      "\tEpoch 8 | Batch 0 | Loss   0.39\n",
      "\tEpoch 8 | Batch 40 | Loss   0.38\n",
      "\tEpoch 8 | Batch 80 | Loss   0.32\n",
      "\tEpoch 8 | Batch 120 | Loss   0.38\n",
      "Epoch 8 | Loss   0.36\n",
      "\tEpoch 9 | Batch 0 | Loss   0.34\n",
      "\tEpoch 9 | Batch 40 | Loss   0.33\n",
      "\tEpoch 9 | Batch 80 | Loss   0.37\n",
      "\tEpoch 9 | Batch 120 | Loss   0.34\n",
      "Epoch 9 | Loss   0.36\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_num % 40 == 0:\n",
    "            print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "    print('Epoch %d | Loss %6.2f' % (epoch, sum(losses)/len(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tensor([]).to(device)\n",
    "for x_label in test_loader:\n",
    "    x = x_label.to(device)\n",
    "    \n",
    "    output = torch.cat((output,model(x.to(device)).to(device).argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8428 0.8436 0.8432\n"
     ]
    }
   ],
   "source": [
    "eval(word2vec_mean_y_test,output.to('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 B\n",
    "\n",
    "https://stackoverflow.com/questions/72480289/how-to-handle-keyerrorfkey-key-not-present-wor2vec-with-gensim\n",
    "https://stackoverflow.com/questions/65372032/deal-with-out-of-vocabulary-word-with-gensim-pretrained-glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_first_10(s:any):\n",
    "    s = str(s).split()[:10]\n",
    "    vector_first_10 = []\n",
    "\n",
    "    for word in str(s).split():\n",
    "        try:\n",
    "            current_vec = wv.get_vector(word)\n",
    "        except:\n",
    "            current_vec = wv.get_vector('unknown')\n",
    "        # print(current_vec)\n",
    "        vector_first_10 = np.concatenate((vector_first_10, current_vec))\n",
    "    \n",
    "    vector_first_10 = np.pad(vector_first_10,(0, (10 - len(s))*300 ))\n",
    "    # if len(vector_first_10) != 3000:\n",
    "    #     print(s)\n",
    "    return vector_first_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean vectors from word2vec pretrained model\n",
    "reviews_ratings_final['word2vec_first_10'] = reviews_ratings_final['review_headline_body'].apply(get_word2vec_first_10)\n",
    "\n",
    "## Approx run time - 2 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "word2vec_first_10_x_train, word2vec_first_10_x_test, word2vec_first_10_y_train, word2vec_first_10_y_test = train_test_split(np.stack(reviews_ratings_final['word2vec_first_10'].values), reviews_ratings_final[\"Class\"].values,test_size=0.2,random_state=34) # Setting the random state to reproduce the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_first_10 = TrainDataset(word2vec_first_10_x_train, word2vec_first_10_y_train)\n",
    "test_set_first_10  = TestDataset(word2vec_first_10_x_test, word2vec_first_10_y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader_first_10 = DataLoader(train_set_first_10, batch_size=batch_size, shuffle=True)\n",
    "test_loader_first_10  = DataLoader(test_set_first_10,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN_perceptron_first_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFN_perceptron_first_10, self).__init__()\n",
    "        self.sequntial = nn.Sequential(nn.Linear(3000, 50), nn.ReLU(), nn.Linear(50,5), nn.ReLU(), nn.Linear(5,3))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.sequntial(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model_first_10 = FFN_perceptron_first_10().to(device)\n",
    "optimizer = torch.optim.Adam(model_first_10.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 | Batch 0 | Loss   1.05\n",
      "\tEpoch 0 | Batch 40 | Loss   0.70\n",
      "\tEpoch 0 | Batch 80 | Loss   0.70\n",
      "\tEpoch 0 | Batch 120 | Loss   0.70\n",
      "\tEpoch 0 | Batch 160 | Loss   0.70\n",
      "\tEpoch 0 | Batch 200 | Loss   0.70\n",
      "\tEpoch 0 | Batch 240 | Loss   0.70\n",
      "\tEpoch 0 | Batch 280 | Loss   0.69\n",
      "Epoch 0 | Loss   0.71 \n",
      "\n",
      "\tEpoch 1 | Batch 0 | Loss   0.70\n",
      "\tEpoch 1 | Batch 40 | Loss   0.69\n",
      "\tEpoch 1 | Batch 80 | Loss   0.69\n",
      "\tEpoch 1 | Batch 120 | Loss   0.70\n",
      "\tEpoch 1 | Batch 160 | Loss   0.69\n",
      "\tEpoch 1 | Batch 200 | Loss   0.69\n",
      "\tEpoch 1 | Batch 240 | Loss   0.69\n",
      "\tEpoch 1 | Batch 280 | Loss   0.69\n",
      "Epoch 1 | Loss   0.69 \n",
      "\n",
      "\tEpoch 2 | Batch 0 | Loss   0.69\n",
      "\tEpoch 2 | Batch 40 | Loss   0.69\n",
      "\tEpoch 2 | Batch 80 | Loss   0.69\n",
      "\tEpoch 2 | Batch 120 | Loss   0.69\n",
      "\tEpoch 2 | Batch 160 | Loss   0.69\n",
      "\tEpoch 2 | Batch 200 | Loss   0.70\n",
      "\tEpoch 2 | Batch 240 | Loss   0.69\n",
      "\tEpoch 2 | Batch 280 | Loss   0.70\n",
      "Epoch 2 | Loss   0.69 \n",
      "\n",
      "\tEpoch 3 | Batch 0 | Loss   0.69\n",
      "\tEpoch 3 | Batch 40 | Loss   0.69\n",
      "\tEpoch 3 | Batch 80 | Loss   0.69\n",
      "\tEpoch 3 | Batch 120 | Loss   0.70\n",
      "\tEpoch 3 | Batch 160 | Loss   0.70\n",
      "\tEpoch 3 | Batch 200 | Loss   0.69\n",
      "\tEpoch 3 | Batch 240 | Loss   0.69\n",
      "\tEpoch 3 | Batch 280 | Loss   0.69\n",
      "Epoch 3 | Loss   0.69 \n",
      "\n",
      "\tEpoch 4 | Batch 0 | Loss   0.69\n",
      "\tEpoch 4 | Batch 40 | Loss   0.69\n",
      "\tEpoch 4 | Batch 80 | Loss   0.69\n",
      "\tEpoch 4 | Batch 120 | Loss   0.69\n",
      "\tEpoch 4 | Batch 160 | Loss   0.70\n",
      "\tEpoch 4 | Batch 200 | Loss   0.69\n",
      "\tEpoch 4 | Batch 240 | Loss   0.69\n",
      "\tEpoch 4 | Batch 280 | Loss   0.69\n",
      "Epoch 4 | Loss   0.69 \n",
      "\n",
      "\tEpoch 5 | Batch 0 | Loss   0.69\n",
      "\tEpoch 5 | Batch 40 | Loss   0.70\n",
      "\tEpoch 5 | Batch 80 | Loss   0.70\n",
      "\tEpoch 5 | Batch 120 | Loss   0.69\n",
      "\tEpoch 5 | Batch 160 | Loss   0.70\n",
      "\tEpoch 5 | Batch 200 | Loss   0.69\n",
      "\tEpoch 5 | Batch 240 | Loss   0.69\n",
      "\tEpoch 5 | Batch 280 | Loss   0.69\n",
      "Epoch 5 | Loss   0.69 \n",
      "\n",
      "\tEpoch 6 | Batch 0 | Loss   0.69\n",
      "\tEpoch 6 | Batch 40 | Loss   0.69\n",
      "\tEpoch 6 | Batch 80 | Loss   0.69\n",
      "\tEpoch 6 | Batch 120 | Loss   0.69\n",
      "\tEpoch 6 | Batch 160 | Loss   0.69\n",
      "\tEpoch 6 | Batch 200 | Loss   0.70\n",
      "\tEpoch 6 | Batch 240 | Loss   0.70\n",
      "\tEpoch 6 | Batch 280 | Loss   0.69\n",
      "Epoch 6 | Loss   0.69 \n",
      "\n",
      "\tEpoch 7 | Batch 0 | Loss   0.69\n",
      "\tEpoch 7 | Batch 40 | Loss   0.69\n",
      "\tEpoch 7 | Batch 80 | Loss   0.69\n",
      "\tEpoch 7 | Batch 120 | Loss   0.69\n",
      "\tEpoch 7 | Batch 160 | Loss   0.69\n",
      "\tEpoch 7 | Batch 200 | Loss   0.70\n",
      "\tEpoch 7 | Batch 240 | Loss   0.69\n",
      "\tEpoch 7 | Batch 280 | Loss   0.69\n",
      "Epoch 7 | Loss   0.69 \n",
      "\n",
      "\tEpoch 8 | Batch 0 | Loss   0.69\n",
      "\tEpoch 8 | Batch 40 | Loss   0.69\n",
      "\tEpoch 8 | Batch 80 | Loss   0.69\n",
      "\tEpoch 8 | Batch 120 | Loss   0.69\n",
      "\tEpoch 8 | Batch 160 | Loss   0.70\n",
      "\tEpoch 8 | Batch 200 | Loss   0.69\n",
      "\tEpoch 8 | Batch 240 | Loss   0.69\n",
      "\tEpoch 8 | Batch 280 | Loss   0.70\n",
      "Epoch 8 | Loss   0.69 \n",
      "\n",
      "\tEpoch 9 | Batch 0 | Loss   0.69\n",
      "\tEpoch 9 | Batch 40 | Loss   0.69\n",
      "\tEpoch 9 | Batch 80 | Loss   0.70\n",
      "\tEpoch 9 | Batch 120 | Loss   0.69\n",
      "\tEpoch 9 | Batch 160 | Loss   0.69\n",
      "\tEpoch 9 | Batch 200 | Loss   0.69\n",
      "\tEpoch 9 | Batch 240 | Loss   0.69\n",
      "\tEpoch 9 | Batch 280 | Loss   0.69\n",
      "Epoch 9 | Loss   0.69 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "model_first_10.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader_first_10):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = input_data\n",
    "        # print(x)\n",
    "        # print(y)\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model_first_10(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_num % 40 == 0:\n",
    "            print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "    print('Epoch %d | Loss %6.2f \\n' % (epoch, sum(losses)/len(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Applied NLP\\HW3\\HW3.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Applied%20NLP/HW3/HW3.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output_first_10 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Applied%20NLP/HW3/HW3.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m x_label \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Applied%20NLP/HW3/HW3.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     x \u001b[39m=\u001b[39m x_label\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Applied%20NLP/HW3/HW3.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     output_first_10 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((output_first_10,model(x\u001b[39m.\u001b[39mto(device))\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "output_first_10 = torch.tensor([]).to(device)\n",
    "for x_label in test_loader:\n",
    "    x = x_label.to(device)\n",
    "    \n",
    "    output_first_10 = torch.cat((output_first_10,model(x.to(device)).to(device).argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8428 0.8436 0.8432\n"
     ]
    }
   ],
   "source": [
    "eval(word2vec_mean_y_test,output_first_10.to('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Recurrent Neural Networks\n",
    "\n",
    "#### a) Simple RNN\n",
    "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn1 = nn.RNN(input_size, hidden_size1, batch_first=True)\n",
    "        \n",
    "        # self.act1 = nn.ReLU()\n",
    "\n",
    "        self.rnn2 =  nn.RNN(hidden_size1, output_size,  batch_first=True)\n",
    "\n",
    "        self.act2 = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out,_ = self.rnn1(x)\n",
    "        # out = self.act1(out)\n",
    "        out,_ = self.rnn2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3000  # You may need to adjust this based on your actual input data shape\n",
    "hidden_size1 = 10\n",
    "output_size = 2\n",
    "\n",
    "model = RNN(input_size, hidden_size1, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_9376\\313490578.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.act2(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 | Batch 0 | Loss   0.72\n",
      "\tEpoch 0 | Batch 40 | Loss   0.69\n",
      "\tEpoch 0 | Batch 80 | Loss   0.69\n",
      "\tEpoch 0 | Batch 120 | Loss   0.67\n",
      "\tEpoch 0 | Batch 160 | Loss   0.67\n",
      "\tEpoch 0 | Batch 200 | Loss   0.69\n",
      "\tEpoch 0 | Batch 240 | Loss   0.66\n",
      "\tEpoch 0 | Batch 280 | Loss   0.69\n",
      "Epoch 0 | Loss   0.68 \n",
      "\n",
      "\tEpoch 1 | Batch 0 | Loss   0.67\n",
      "\tEpoch 1 | Batch 40 | Loss   0.68\n",
      "\tEpoch 1 | Batch 80 | Loss   0.68\n",
      "\tEpoch 1 | Batch 120 | Loss   0.68\n",
      "\tEpoch 1 | Batch 160 | Loss   0.67\n",
      "\tEpoch 1 | Batch 200 | Loss   0.67\n",
      "\tEpoch 1 | Batch 240 | Loss   0.68\n",
      "\tEpoch 1 | Batch 280 | Loss   0.68\n",
      "Epoch 1 | Loss   0.68 \n",
      "\n",
      "\tEpoch 2 | Batch 0 | Loss   0.67\n",
      "\tEpoch 2 | Batch 40 | Loss   0.67\n",
      "\tEpoch 2 | Batch 80 | Loss   0.67\n",
      "\tEpoch 2 | Batch 120 | Loss   0.67\n",
      "\tEpoch 2 | Batch 160 | Loss   0.68\n",
      "\tEpoch 2 | Batch 200 | Loss   0.66\n",
      "\tEpoch 2 | Batch 240 | Loss   0.67\n",
      "\tEpoch 2 | Batch 280 | Loss   0.68\n",
      "Epoch 2 | Loss   0.68 \n",
      "\n",
      "\tEpoch 3 | Batch 0 | Loss   0.68\n",
      "\tEpoch 3 | Batch 40 | Loss   0.67\n",
      "\tEpoch 3 | Batch 80 | Loss   0.69\n",
      "\tEpoch 3 | Batch 120 | Loss   0.68\n",
      "\tEpoch 3 | Batch 160 | Loss   0.68\n",
      "\tEpoch 3 | Batch 200 | Loss   0.68\n",
      "\tEpoch 3 | Batch 240 | Loss   0.68\n",
      "\tEpoch 3 | Batch 280 | Loss   0.66\n",
      "Epoch 3 | Loss   0.68 \n",
      "\n",
      "\tEpoch 4 | Batch 0 | Loss   0.69\n",
      "\tEpoch 4 | Batch 40 | Loss   0.67\n",
      "\tEpoch 4 | Batch 80 | Loss   0.67\n",
      "\tEpoch 4 | Batch 120 | Loss   0.67\n",
      "\tEpoch 4 | Batch 160 | Loss   0.68\n",
      "\tEpoch 4 | Batch 200 | Loss   0.67\n",
      "\tEpoch 4 | Batch 240 | Loss   0.68\n",
      "\tEpoch 4 | Batch 280 | Loss   0.68\n",
      "Epoch 4 | Loss   0.68 \n",
      "\n",
      "\tEpoch 5 | Batch 0 | Loss   0.68\n",
      "\tEpoch 5 | Batch 40 | Loss   0.68\n",
      "\tEpoch 5 | Batch 80 | Loss   0.67\n",
      "\tEpoch 5 | Batch 120 | Loss   0.68\n",
      "\tEpoch 5 | Batch 160 | Loss   0.68\n",
      "\tEpoch 5 | Batch 200 | Loss   0.67\n",
      "\tEpoch 5 | Batch 240 | Loss   0.68\n",
      "\tEpoch 5 | Batch 280 | Loss   0.66\n",
      "Epoch 5 | Loss   0.68 \n",
      "\n",
      "\tEpoch 6 | Batch 0 | Loss   0.68\n",
      "\tEpoch 6 | Batch 40 | Loss   0.67\n",
      "\tEpoch 6 | Batch 80 | Loss   0.67\n",
      "\tEpoch 6 | Batch 120 | Loss   0.67\n",
      "\tEpoch 6 | Batch 160 | Loss   0.67\n",
      "\tEpoch 6 | Batch 200 | Loss   0.68\n",
      "\tEpoch 6 | Batch 240 | Loss   0.67\n",
      "\tEpoch 6 | Batch 280 | Loss   0.68\n",
      "Epoch 6 | Loss   0.68 \n",
      "\n",
      "\tEpoch 7 | Batch 0 | Loss   0.67\n",
      "\tEpoch 7 | Batch 40 | Loss   0.69\n",
      "\tEpoch 7 | Batch 80 | Loss   0.66\n",
      "\tEpoch 7 | Batch 120 | Loss   0.68\n",
      "\tEpoch 7 | Batch 160 | Loss   0.69\n",
      "\tEpoch 7 | Batch 200 | Loss   0.67\n",
      "\tEpoch 7 | Batch 240 | Loss   0.68\n",
      "\tEpoch 7 | Batch 280 | Loss   0.67\n",
      "Epoch 7 | Loss   0.68 \n",
      "\n",
      "\tEpoch 8 | Batch 0 | Loss   0.68\n",
      "\tEpoch 8 | Batch 40 | Loss   0.67\n",
      "\tEpoch 8 | Batch 80 | Loss   0.67\n",
      "\tEpoch 8 | Batch 120 | Loss   0.69\n",
      "\tEpoch 8 | Batch 160 | Loss   0.68\n",
      "\tEpoch 8 | Batch 200 | Loss   0.65\n",
      "\tEpoch 8 | Batch 240 | Loss   0.68\n",
      "\tEpoch 8 | Batch 280 | Loss   0.70\n",
      "Epoch 8 | Loss   0.68 \n",
      "\n",
      "\tEpoch 9 | Batch 0 | Loss   0.67\n",
      "\tEpoch 9 | Batch 40 | Loss   0.66\n",
      "\tEpoch 9 | Batch 80 | Loss   0.67\n",
      "\tEpoch 9 | Batch 120 | Loss   0.68\n",
      "\tEpoch 9 | Batch 160 | Loss   0.67\n",
      "\tEpoch 9 | Batch 200 | Loss   0.66\n",
      "\tEpoch 9 | Batch 240 | Loss   0.69\n",
      "\tEpoch 9 | Batch 280 | Loss   0.69\n",
      "Epoch 9 | Loss   0.68 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader_first_10):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_num % 40 == 0:\n",
    "            print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "    print('Epoch %d | Loss %6.2f \\n' % (epoch, sum(losses)/len(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_9376\\313490578.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.act2(out)\n"
     ]
    }
   ],
   "source": [
    "output_first_10 = torch.tensor([]).to(device)\n",
    "for x_label in test_loader_first_10:\n",
    "    x = x_label.to(device).float()\n",
    "    \n",
    "    output_first_10 = torch.cat((output_first_10,model(x.to(device)).to(device).argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7331 0.1894 0.301\n"
     ]
    }
   ],
   "source": [
    "eval(word2vec_first_10_y_test,output_first_10.to('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_gated(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, output_size):\n",
    "        super(RNN_gated, self).__init__()\n",
    "\n",
    "        self.rnn1 = nn.GRU(input_size, hidden_size1, batch_first=True)\n",
    "        \n",
    "        # self.act1 = nn.ReLU()\n",
    "\n",
    "        self.rnn2 =  nn.GRU(hidden_size1, output_size,  batch_first=True)\n",
    "\n",
    "        self.act2 = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out,_ = self.rnn1(x)\n",
    "        # out = self.act1(out)\n",
    "        out,_ = self.rnn2(out)\n",
    "        out = self.act2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3000  # You may need to adjust this based on your actual input data shape\n",
    "hidden_size1 = 10\n",
    "output_size = 2\n",
    "\n",
    "model = RNN_gated(input_size, hidden_size1, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_9376\\1520829041.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.act2(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 | Batch 0 | Loss   0.70\n",
      "\tEpoch 0 | Batch 40 | Loss   0.69\n",
      "\tEpoch 0 | Batch 80 | Loss   0.68\n",
      "\tEpoch 0 | Batch 120 | Loss   0.68\n",
      "\tEpoch 0 | Batch 160 | Loss   0.69\n",
      "\tEpoch 0 | Batch 200 | Loss   0.69\n",
      "\tEpoch 0 | Batch 240 | Loss   0.68\n",
      "\tEpoch 0 | Batch 280 | Loss   0.68\n",
      "Epoch 0 | Loss   0.68 \n",
      "\n",
      "\tEpoch 1 | Batch 0 | Loss   0.67\n",
      "\tEpoch 1 | Batch 40 | Loss   0.67\n",
      "\tEpoch 1 | Batch 80 | Loss   0.68\n",
      "\tEpoch 1 | Batch 120 | Loss   0.67\n",
      "\tEpoch 1 | Batch 160 | Loss   0.68\n",
      "\tEpoch 1 | Batch 200 | Loss   0.68\n",
      "\tEpoch 1 | Batch 240 | Loss   0.68\n",
      "\tEpoch 1 | Batch 280 | Loss   0.67\n",
      "Epoch 1 | Loss   0.68 \n",
      "\n",
      "\tEpoch 2 | Batch 0 | Loss   0.68\n",
      "\tEpoch 2 | Batch 40 | Loss   0.67\n",
      "\tEpoch 2 | Batch 80 | Loss   0.68\n",
      "\tEpoch 2 | Batch 120 | Loss   0.69\n",
      "\tEpoch 2 | Batch 160 | Loss   0.67\n",
      "\tEpoch 2 | Batch 200 | Loss   0.68\n",
      "\tEpoch 2 | Batch 240 | Loss   0.68\n",
      "\tEpoch 2 | Batch 280 | Loss   0.69\n",
      "Epoch 2 | Loss   0.68 \n",
      "\n",
      "\tEpoch 3 | Batch 0 | Loss   0.67\n",
      "\tEpoch 3 | Batch 40 | Loss   0.68\n",
      "\tEpoch 3 | Batch 80 | Loss   0.67\n",
      "\tEpoch 3 | Batch 120 | Loss   0.70\n",
      "\tEpoch 3 | Batch 160 | Loss   0.68\n",
      "\tEpoch 3 | Batch 200 | Loss   0.68\n",
      "\tEpoch 3 | Batch 240 | Loss   0.68\n",
      "\tEpoch 3 | Batch 280 | Loss   0.68\n",
      "Epoch 3 | Loss   0.68 \n",
      "\n",
      "\tEpoch 4 | Batch 0 | Loss   0.67\n",
      "\tEpoch 4 | Batch 40 | Loss   0.67\n",
      "\tEpoch 4 | Batch 80 | Loss   0.68\n",
      "\tEpoch 4 | Batch 120 | Loss   0.67\n",
      "\tEpoch 4 | Batch 160 | Loss   0.68\n",
      "\tEpoch 4 | Batch 200 | Loss   0.67\n",
      "\tEpoch 4 | Batch 240 | Loss   0.68\n",
      "\tEpoch 4 | Batch 280 | Loss   0.67\n",
      "Epoch 4 | Loss   0.68 \n",
      "\n",
      "\tEpoch 5 | Batch 0 | Loss   0.67\n",
      "\tEpoch 5 | Batch 40 | Loss   0.67\n",
      "\tEpoch 5 | Batch 80 | Loss   0.68\n",
      "\tEpoch 5 | Batch 120 | Loss   0.67\n",
      "\tEpoch 5 | Batch 160 | Loss   0.69\n",
      "\tEpoch 5 | Batch 200 | Loss   0.69\n",
      "\tEpoch 5 | Batch 240 | Loss   0.68\n",
      "\tEpoch 5 | Batch 280 | Loss   0.68\n",
      "Epoch 5 | Loss   0.68 \n",
      "\n",
      "\tEpoch 6 | Batch 0 | Loss   0.67\n",
      "\tEpoch 6 | Batch 40 | Loss   0.69\n",
      "\tEpoch 6 | Batch 80 | Loss   0.69\n",
      "\tEpoch 6 | Batch 120 | Loss   0.67\n",
      "\tEpoch 6 | Batch 160 | Loss   0.67\n",
      "\tEpoch 6 | Batch 200 | Loss   0.67\n",
      "\tEpoch 6 | Batch 240 | Loss   0.67\n",
      "\tEpoch 6 | Batch 280 | Loss   0.67\n",
      "Epoch 6 | Loss   0.68 \n",
      "\n",
      "\tEpoch 7 | Batch 0 | Loss   0.70\n",
      "\tEpoch 7 | Batch 40 | Loss   0.66\n",
      "\tEpoch 7 | Batch 80 | Loss   0.68\n",
      "\tEpoch 7 | Batch 120 | Loss   0.70\n",
      "\tEpoch 7 | Batch 160 | Loss   0.69\n",
      "\tEpoch 7 | Batch 200 | Loss   0.68\n",
      "\tEpoch 7 | Batch 240 | Loss   0.69\n",
      "\tEpoch 7 | Batch 280 | Loss   0.69\n",
      "Epoch 7 | Loss   0.68 \n",
      "\n",
      "\tEpoch 8 | Batch 0 | Loss   0.68\n",
      "\tEpoch 8 | Batch 40 | Loss   0.68\n",
      "\tEpoch 8 | Batch 80 | Loss   0.68\n",
      "\tEpoch 8 | Batch 120 | Loss   0.68\n",
      "\tEpoch 8 | Batch 160 | Loss   0.69\n",
      "\tEpoch 8 | Batch 200 | Loss   0.67\n",
      "\tEpoch 8 | Batch 240 | Loss   0.67\n",
      "\tEpoch 8 | Batch 280 | Loss   0.68\n",
      "Epoch 8 | Loss   0.68 \n",
      "\n",
      "\tEpoch 9 | Batch 0 | Loss   0.68\n",
      "\tEpoch 9 | Batch 40 | Loss   0.67\n",
      "\tEpoch 9 | Batch 80 | Loss   0.67\n",
      "\tEpoch 9 | Batch 120 | Loss   0.69\n",
      "\tEpoch 9 | Batch 160 | Loss   0.68\n",
      "\tEpoch 9 | Batch 200 | Loss   0.69\n",
      "\tEpoch 9 | Batch 240 | Loss   0.70\n",
      "\tEpoch 9 | Batch 280 | Loss   0.67\n",
      "Epoch 9 | Loss   0.68 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader_first_10):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_num % 40 == 0:\n",
    "            print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "    print('Epoch %d | Loss %6.2f \\n' % (epoch, sum(losses)/len(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_9376\\1520829041.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = self.act2(out)\n"
     ]
    }
   ],
   "source": [
    "output_first_10 = torch.tensor([]).to(device)\n",
    "for x_label in test_loader_first_10:\n",
    "    x = x_label.to(device).float()\n",
    "    \n",
    "    output_first_10 = torch.cat((output_first_10,model(x.to(device)).to(device).argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7331 0.1894 0.301\n"
     ]
    }
   ],
   "source": [
    "eval(word2vec_first_10_y_test,output_first_10.to('cpu'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
